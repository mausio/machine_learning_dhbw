{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35f93443",
   "metadata": {},
   "source": [
    "# Solution 07: Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5984c69f-bb7d-4bbd-b02f-b72284fa28d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T17:26:22.231115Z",
     "start_time": "2024-11-24T17:26:22.226955Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "pd.options.display.max_columns = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfaf7fef-4333-4009-ac69-9142ad497208",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T17:26:25.262989Z",
     "start_time": "2024-11-24T17:26:22.232839Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/immo_data.csv\")\n",
    "desc = pd.read_csv(\"../data/immo_data_column_description.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4851f20-f4c7-4ab3-b727-ce537c55157a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T17:26:25.268963Z",
     "start_time": "2024-11-24T17:26:25.263845Z"
    }
   },
   "outputs": [],
   "source": [
    "def drop_columns(df):\n",
    "    \"\"\" Remove (supposedly) unimportant columns \"\"\"\n",
    "    return df.drop(\n",
    "        [\n",
    "            \"scoutId\",\n",
    "            \"houseNumber\",\n",
    "            \"geo_bln\",\n",
    "            \"geo_krs\",\n",
    "            \"geo_plz\",\n",
    "            \"date\",\n",
    "            \"street\",\n",
    "            \"streetPlain\",\n",
    "            \"description\",\n",
    "            \"facilities\",\n",
    "            \"regio3\",\n",
    "            \"firingTypes\",\n",
    "            \"telekomHybridUploadSpeed\",\n",
    "            \"totalRent\",\n",
    "            \"baseRentRange\",\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "\n",
    "def remove_outliers(df, lower_limit=0.005, upper_limit=0.995):\n",
    "    \"\"\" Removing the (lower and upper) outliers \"\"\"\n",
    "    dfc = df.copy()\n",
    "    columns_with_outliers = [\n",
    "        \"serviceCharge\",\n",
    "        \"yearConstructed\",\n",
    "        \"noParkSpaces\",\n",
    "        \"baseRent\",\n",
    "        \"livingSpace\",\n",
    "        \"noRooms\",\n",
    "        \"floor\",\n",
    "        \"numberOfFloors\",\n",
    "        \"heatingCosts\",\n",
    "        \"lastRefurbish\",\n",
    "    ]\n",
    "    \n",
    "    # For each column we keep: Data that are < (99.5% quantile) and > (0.5% quantile) OR that are NaN (we will deal with this later). \n",
    "    upper_limits = df[columns_with_outliers].quantile(upper_limit)\n",
    "    lower_limits = df[columns_with_outliers].quantile(lower_limit)\n",
    "    \n",
    "    for colname in columns_with_outliers:\n",
    "        col = dfc[colname]\n",
    "        dfc = dfc[\n",
    "            ((col <= upper_limits[colname]) & (col >= lower_limits[colname]))\n",
    "            | col.isna()\n",
    "        ]\n",
    "    return dfc\n",
    "\n",
    "\n",
    "def remove_rows_with_NaN_target(df):\n",
    "    \"\"\" Removing the records without a label \"\"\"\n",
    "    return df[df[\"baseRent\"].isna() == False]\n",
    "\n",
    "\n",
    "def impute_NaNs(df):\n",
    "    \"\"\" Replacing NaNs with mean or most frequent \"\"\"\n",
    "    dfc = df.copy()\n",
    "    categorical_columns = dfc.select_dtypes(exclude=np.number).columns\n",
    "    imp_freq = SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\")\n",
    "    dfc[categorical_columns] = imp_freq.fit_transform(dfc[categorical_columns])\n",
    "\n",
    "    numeric_columns = dfc.select_dtypes(include=np.number).columns\n",
    "    imp_mean = SimpleImputer(missing_values=np.nan, strategy=\"mean\")\n",
    "    dfc[numeric_columns] = imp_mean.fit_transform(dfc[numeric_columns])\n",
    "    return dfc\n",
    "\n",
    "\n",
    "def print_evaluation(pipeline_or_model, X_train, X_test, y_train, y_test, y_train_pred, y_test_pred, feature_names, show_coeff=True):\n",
    "    \"\"\" Output of R2 value, MSE and MAE for training and test set \"\"\"\n",
    "    r2_train = r2_score(y_train, y_train_pred)\n",
    "    mse_train = math.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "\n",
    "    r2_test = r2_score(y_test, y_test_pred)\n",
    "    mse_test = math.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "    \n",
    "    print(\n",
    "        f\"{pipeline_or_model} Evaluation:\\n\"\n",
    "        f\"{'':6} {'R²':>10} | {'MSE':>14} | {'MAE':>10} | {'rows':>8} | {'columns':>8}\\n\"\n",
    "        f\"{'Train':6} {r2_train:10.5f} | {mse_train:14.2f} | {mae_train:10.2f} | {X_train.shape[0]:8} | {X_train.shape[1]:8}\\n\"\n",
    "        f\"{'Test':6} {r2_test:10.5f} | {mse_test:14.2f} | {mae_test:10.2f} | {X_test.shape[0]:8} | {X_test.shape[1]:8}\\n\"\n",
    "    )\n",
    "    \n",
    "    # Output of the first 10 coefficients, scaled in descending order by absolute value\n",
    "    coefficients_lr = pd.DataFrame({\"Feature Name\": feature_names, \"Coefficient\": pipeline_or_model.coef_})\n",
    "    if show_coeff:\n",
    "        display(coefficients_lr.sort_values(\"Coefficient\", key=abs, ascending=False).head(10))\n",
    "    \n",
    "    # How many coefficients are non-zero?\n",
    "    nonzero_coeff = sum(abs(coefficients_lr[\"Coefficient\"])>1e-12)\n",
    "    print(f\"Number of nonzero coefficients: {nonzero_coeff}/{X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4739fb-6539-415c-98b1-80334e22be23",
   "metadata": {},
   "source": [
    "## Task 1 \n",
    "Observation: with increasing $\\alpha$ the error on the training data increases. In this data set, the problem of immens error on the test data is already eliminated with a very small value for $\\alpha$. The larger we make $\\alpha$, the larger errors on training AND test data will eventually become. With the $\\ell_1$-regularization (LASSO), however, more and more coefficients are equal to zero, i.e. a model with few features (=easier to understand for humans) already has a high predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "016fed6a-366d-41a2-83bc-e012c697c8c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T17:26:44.524401Z",
     "start_time": "2024-11-24T17:26:25.270790Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.linear_model._ridge.Ridge'>, alpha = 0.0\n",
      "Ridge(alpha=0.0) Evaluation:\n",
      "               R² |            MSE |        MAE |     rows |  columns\n",
      "Train     0.82795 |         167.97 |     114.18 |     7742 |      511\n",
      "Test   -1024275060639381195849728.00000 | 407179081533658.12 | 26006931164852.94 |     1936 |      511\n",
      "\n",
      "Number of nonzero coefficients: 511/511\n",
      "--------------------------------------------\n",
      "<class 'sklearn.linear_model._ridge.Ridge'>, alpha = 0.08798863562983575\n",
      "Ridge(alpha=0.08798863562983575) Evaluation:\n",
      "               R² |            MSE |        MAE |     rows |  columns\n",
      "Train     0.84461 |         159.62 |     103.83 |     7742 |      511\n",
      "Test      0.82224 |         169.62 |     114.30 |     1936 |      511\n",
      "\n",
      "Number of nonzero coefficients: 502/511\n",
      "--------------------------------------------\n",
      "<class 'sklearn.linear_model._ridge.Ridge'>, alpha = 0.8798863562983574\n",
      "Ridge(alpha=0.8798863562983574) Evaluation:\n",
      "               R² |            MSE |        MAE |     rows |  columns\n",
      "Train     0.84461 |         159.62 |     103.82 |     7742 |      511\n",
      "Test      0.82225 |         169.62 |     114.29 |     1936 |      511\n",
      "\n",
      "Number of nonzero coefficients: 502/511\n",
      "--------------------------------------------\n",
      "<class 'sklearn.linear_model._ridge.Ridge'>, alpha = 8.798863562983575\n",
      "Ridge(alpha=8.798863562983575) Evaluation:\n",
      "               R² |            MSE |        MAE |     rows |  columns\n",
      "Train     0.84460 |         159.63 |     103.80 |     7742 |      511\n",
      "Test      0.82231 |         169.59 |     114.25 |     1936 |      511\n",
      "\n",
      "Number of nonzero coefficients: 502/511\n",
      "--------------------------------------------\n",
      "<class 'sklearn.linear_model._ridge.Ridge'>, alpha = 87.98863562983574\n",
      "Ridge(alpha=87.98863562983574) Evaluation:\n",
      "               R² |            MSE |        MAE |     rows |  columns\n",
      "Train     0.84396 |         159.96 |     103.84 |     7742 |      511\n",
      "Test      0.82216 |         169.66 |     114.13 |     1936 |      511\n",
      "\n",
      "Number of nonzero coefficients: 502/511\n",
      "--------------------------------------------\n",
      "<class 'sklearn.linear_model._ridge.Ridge'>, alpha = 879.8863562983574\n",
      "Ridge(alpha=879.8863562983574) Evaluation:\n",
      "               R² |            MSE |        MAE |     rows |  columns\n",
      "Train     0.83630 |         163.84 |     105.13 |     7742 |      511\n",
      "Test      0.81681 |         172.20 |     114.48 |     1936 |      511\n",
      "\n",
      "Number of nonzero coefficients: 502/511\n",
      "--------------------------------------------\n",
      "<class 'sklearn.linear_model._ridge.Ridge'>, alpha = 8798.863562983574\n",
      "Ridge(alpha=8798.863562983574) Evaluation:\n",
      "               R² |            MSE |        MAE |     rows |  columns\n",
      "Train     0.76551 |         196.09 |     121.95 |     7742 |      511\n",
      "Test      0.75739 |         198.17 |     126.55 |     1936 |      511\n",
      "\n",
      "Number of nonzero coefficients: 502/511\n",
      "--------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5b/zrc1vq6j1gg62hcmsxkpdr4m0000gn/T/ipykernel_40818/53933199.py:28: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  model_lr.fit(X_train, y_train)\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:631: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.863e+07, tolerance: 1.269e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.linear_model._coordinate_descent.Lasso'>, alpha = 0\n",
      "Lasso(alpha=0) Evaluation:\n",
      "               R² |            MSE |        MAE |     rows |  columns\n",
      "Train     0.84461 |         159.62 |     103.83 |     7742 |      511\n",
      "Test      0.82104 |         170.20 |     114.65 |     1936 |      511\n",
      "\n",
      "Number of nonzero coefficients: 498/511\n",
      "--------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.890e+07, tolerance: 1.269e+05\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.linear_model._coordinate_descent.Lasso'>, alpha = 0.001\n",
      "Lasso(alpha=0.001) Evaluation:\n",
      "               R² |            MSE |        MAE |     rows |  columns\n",
      "Train     0.84461 |         159.62 |     103.82 |     7742 |      511\n",
      "Test      0.82136 |         170.05 |     114.56 |     1936 |      511\n",
      "\n",
      "Number of nonzero coefficients: 497/511\n",
      "--------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.683e+06, tolerance: 1.269e+05\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.linear_model._coordinate_descent.Lasso'>, alpha = 0.01\n",
      "Lasso(alpha=0.01) Evaluation:\n",
      "               R² |            MSE |        MAE |     rows |  columns\n",
      "Train     0.84461 |         159.62 |     103.81 |     7742 |      511\n",
      "Test      0.82246 |         169.52 |     114.21 |     1936 |      511\n",
      "\n",
      "Number of nonzero coefficients: 482/511\n",
      "--------------------------------------------\n",
      "<class 'sklearn.linear_model._coordinate_descent.Lasso'>, alpha = 0.1\n",
      "Lasso(alpha=0.1) Evaluation:\n",
      "               R² |            MSE |        MAE |     rows |  columns\n",
      "Train     0.84457 |         159.64 |     103.72 |     7742 |      511\n",
      "Test      0.82291 |         169.31 |     113.96 |     1936 |      511\n",
      "\n",
      "Number of nonzero coefficients: 462/511\n",
      "--------------------------------------------\n",
      "<class 'sklearn.linear_model._coordinate_descent.Lasso'>, alpha = 1\n",
      "Lasso(alpha=1) Evaluation:\n",
      "               R² |            MSE |        MAE |     rows |  columns\n",
      "Train     0.84186 |         161.03 |     104.09 |     7742 |      511\n",
      "Test      0.82149 |         169.98 |     113.88 |     1936 |      511\n",
      "\n",
      "Number of nonzero coefficients: 361/511\n",
      "--------------------------------------------\n",
      "<class 'sklearn.linear_model._coordinate_descent.Lasso'>, alpha = 10\n",
      "Lasso(alpha=10) Evaluation:\n",
      "               R² |            MSE |        MAE |     rows |  columns\n",
      "Train     0.79401 |         183.78 |     119.78 |     7742 |      511\n",
      "Test      0.78735 |         185.53 |     124.47 |     1936 |      511\n",
      "\n",
      "Number of nonzero coefficients: 43/511\n",
      "--------------------------------------------\n",
      "<class 'sklearn.linear_model._coordinate_descent.Lasso'>, alpha = 100\n",
      "Lasso(alpha=100) Evaluation:\n",
      "               R² |            MSE |        MAE |     rows |  columns\n",
      "Train     0.51136 |         283.06 |     198.02 |     7742 |      511\n",
      "Test      0.50567 |         282.87 |     201.63 |     1936 |      511\n",
      "\n",
      "Number of nonzero coefficients: 4/511\n",
      "--------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Data pre-processing\n",
    "df_reduced = drop_columns(df.sample(10000, random_state=42))\n",
    "df_reduced = remove_outliers(df_reduced)\n",
    "df_reduced = remove_rows_with_NaN_target(df_reduced)\n",
    "df_reduced = impute_NaNs(df_reduced)\n",
    "df_reduced = pd.get_dummies(df_reduced)\n",
    "y = df_reduced.pop(\"baseRent\")\n",
    "\n",
    "# Training-Test-Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_reduced, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "# equivalent: .fit_transform calls .fit first, then .transform\n",
    "#scaler.fit(X_train)\n",
    "#X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Training for difference valus of alpha\n",
    "N = X_train.shape[0]\n",
    "alphas = [0, 1e-3, 1e-2, 1e-1, 1, 10, 100]\n",
    "models = [linear_model.Ridge, linear_model.Lasso]\n",
    "factors = [np.sqrt(N), 1]\n",
    "for model, factor in zip(models, factors):\n",
    "    for alpha in alphas:\n",
    "        model_lr = model(alpha=factor*alpha)\n",
    "        model_lr.fit(X_train, y_train)\n",
    "        y_train_pred = model_lr.predict(X_train)\n",
    "        y_test_pred = model_lr.predict(X_test)\n",
    "\n",
    "        # Evaluation\n",
    "        print(f\"{model}, alpha = {factor*alpha}\")\n",
    "        print_evaluation(model_lr, X_train, X_test, y_train, y_test, y_train_pred, y_test_pred, feature_names=df_reduced.columns, show_coeff=False)\n",
    "        print(\"--------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504f794b-1091-4b8f-ae90-f0544386685b",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b21b18d7-1ee2-431c-bfa4-823b6188504f",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-11-24T17:26:46.850732Z",
     "start_time": "2024-11-24T17:26:44.527127Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha=10 (mean score=0.8186885840835261, sigma=0.013342189818551273\n",
      "Ridge(alpha=10) Evaluation:\n",
      "               R² |            MSE |        MAE |     rows |  columns\n",
      "Train     0.84459 |         159.63 |     103.79 |     7742 |      511\n",
      "Test      0.82232 |         169.59 |     114.24 |     1936 |      511\n",
      "\n",
      "Number of nonzero coefficients: 502/511\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def cross_validation(alphas, X_train, y_train):\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    mean_scores = []\n",
    "    stddev_scores = []\n",
    "    \n",
    "    # Loop over all the alphas we want to try out\n",
    "    for alpha in alphas:\n",
    "        scores = []\n",
    "        # Loop over 5 different training/test splits\n",
    "        for train_index, test_index in kf.split(X_train):\n",
    "            X_trainK, X_testK = X_train[train_index], X_train[test_index]\n",
    "            y_trainK, y_testK = y_train[train_index], y_train[test_index]\n",
    "            m_ridgeK = linear_model.Ridge(alpha=alpha)\n",
    "            m_ridgeK.fit(X_trainK, y_trainK)\n",
    "            score = m_ridgeK.score(X_testK, y_testK)\n",
    "            scores.append(score)\n",
    "        # Saving mean and standard deviation over the 5 runs for each alpha\n",
    "        mean_scores.append(np.mean(np.array(scores)))\n",
    "        stddev_scores.append(np.std(np.array(scores)))\n",
    "    return mean_scores, stddev_scores\n",
    "\n",
    "# Data pre-processing\n",
    "df_reduced = drop_columns(df.sample(10000, random_state=42))\n",
    "df_reduced = remove_outliers(df_reduced)\n",
    "df_reduced = remove_rows_with_NaN_target(df_reduced)\n",
    "df_reduced = impute_NaNs(df_reduced)\n",
    "df_reduced = pd.get_dummies(df_reduced)\n",
    "y = df_reduced.pop(\"baseRent\").values\n",
    "\n",
    "# Training-Test-Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_reduced, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Cross validation (step 2 algorithm)\n",
    "alphas = [1e-3, 1e-2, 1e-1, 1, 10, 100]\n",
    "mean_scores, stddev_scores = cross_validation(alphas, X_train, y_train)\n",
    "\n",
    "# For the best alpha we train again on the full training dataset\n",
    "best_alpha_index = mean_scores.index(max(mean_scores))\n",
    "best_alpha = alphas[best_alpha_index]\n",
    "m_ridge = linear_model.Ridge(alpha=best_alpha)\n",
    "m_ridge.fit(X_train, y_train)\n",
    "y_train_pred = m_ridge.predict(X_train)\n",
    "y_test_pred = m_ridge.predict(X_test)\n",
    "print(f\"Best alpha={best_alpha} (mean score={mean_scores[best_alpha_index]}, sigma={stddev_scores[best_alpha_index]}\")\n",
    "\n",
    "print_evaluation(m_ridge, X_train, X_test, y_train, y_test, y_train_pred, y_test_pred, feature_names=df_reduced.columns, show_coeff=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f7f19b-a3f9-4994-9b1c-3c6b2ecb5d5a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 3- Nested Cross-Validation\n",
    "Here an outer loop around the cross validation is implemented. The purpose is that different test sets are calculated. The average over the errors on the different test sets is now a more reliable estimate for the error on unknown data. One disadvantage is that you end up with multiple models. For the deployment you have to choose one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "956f6144-82df-4b40-9c95-c29dccb01bae",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2024-11-24T17:26:50.699041Z",
     "start_time": "2024-11-24T17:26:46.855250Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha=10 (mean score=0.813919081347251, sigma=0.010418081871166078\n",
      "Ridge(alpha=10) Evaluation:\n",
      "               R² |            MSE |        MAE |     rows |  columns\n",
      "Train     0.84341 |         161.87 |     105.05 |     6452 |      511\n",
      "Test      0.82408 |         165.60 |     112.50 |     3226 |      511\n",
      "\n",
      "Number of nonzero coefficients: 498/511\n",
      "Mean of R² on test sets: 0.8240823199994203\n",
      "Best alpha=10 (mean score=0.8233035341808088, sigma=0.010934817423729495\n",
      "Ridge(alpha=10) Evaluation:\n",
      "               R² |            MSE |        MAE |     rows |  columns\n",
      "Train     0.85149 |         154.99 |     102.29 |     6452 |      511\n",
      "Test      0.81204 |         177.21 |     114.17 |     3226 |      511\n",
      "\n",
      "Number of nonzero coefficients: 504/511\n",
      "Mean of R² on test sets: 0.8180590734560631\n",
      "Best alpha=10 (mean score=0.8127459941121421, sigma=0.012346249054763652\n",
      "Ridge(alpha=10) Evaluation:\n",
      "               R² |            MSE |        MAE |     rows |  columns\n",
      "Train     0.84154 |         160.00 |     103.77 |     6452 |      511\n",
      "Test      0.83119 |         168.18 |     110.22 |     3226 |      511\n",
      "\n",
      "Number of nonzero coefficients: 502/511\n",
      "Mean of R² on test sets: 0.8224364972796145\n"
     ]
    }
   ],
   "source": [
    "# Data pre-processing\n",
    "df_reduced = drop_columns(df.sample(10000, random_state=42))\n",
    "df_reduced = remove_outliers(df_reduced)\n",
    "df_reduced = remove_rows_with_NaN_target(df_reduced)\n",
    "df_reduced = impute_NaNs(df_reduced)\n",
    "df_reduced = pd.get_dummies(df_reduced)\n",
    "y = df_reduced.pop(\"baseRent\").values\n",
    "\n",
    "X = df_reduced.values.astype(float)\n",
    "\n",
    "# Outer loop cross validation: Various test set\n",
    "\n",
    "kf_outer = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "test_scores = []\n",
    "# Loop over 3 different training/test splits\n",
    "for train_index, test_index in kf_outer.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Inner cross validation to determine the best alpha (step 2 algorithm).\n",
    "    alphas = [1e-3, 1e-2, 1e-1, 1, 10, 100]\n",
    "    mean_scores, stddev_scores = cross_validation(alphas, X_train, y_train)\n",
    "\n",
    "    # For the best alpha we train again on the full training dataset\n",
    "    best_alpha_index = mean_scores.index(max(mean_scores))\n",
    "    best_alpha = alphas[best_alpha_index]\n",
    "    m_ridge = linear_model.Ridge(alpha=best_alpha)\n",
    "    m_ridge.fit(X_train, y_train)\n",
    "    y_train_pred = m_ridge.predict(X_train)\n",
    "    y_test_pred = m_ridge.predict(X_test)\n",
    "    test_scores.append(m_ridge.score(X_test, y_test))\n",
    "\n",
    "    print(f\"Best alpha={best_alpha} (mean score={mean_scores[best_alpha_index]}, sigma={stddev_scores[best_alpha_index]}\")\n",
    "    print_evaluation(m_ridge, X_train, X_test, y_train, y_test, y_train_pred, y_test_pred, feature_names=df_reduced.columns, show_coeff=False)\n",
    "    print(f\"Mean of R² on test sets: {np.mean(np.array(test_scores))}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
