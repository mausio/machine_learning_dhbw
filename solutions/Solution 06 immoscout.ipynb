{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "379aefa8-aafe-42fa-a04d-655b0f5c7c2e",
   "metadata": {},
   "source": [
    "# Solution 05 Immoscout\n",
    "Since we will be training and evaluating multiple models in the following and also doing the associated data preprocessing multiple times, the functionality has been encapsulated into (reusable) functions so that they can be used with each model with little code duplication. Furthermore, the plots were removed to make the code clearer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5984c69f-bb7d-4bbd-b02f-b72284fa28d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T15:00:57.644961Z",
     "start_time": "2024-11-21T15:00:57.642477Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn import linear_model\n",
    "\n",
    "from warnings import simplefilter\n",
    "\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "pd.options.display.max_columns = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6c4eb2-738a-4a99-ae53-3716e5aa0778",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfaf7fef-4333-4009-ac69-9142ad497208",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T15:01:00.752430Z",
     "start_time": "2024-11-21T15:00:57.660958Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Data/immo_data.csv\")\n",
    "desc = pd.read_csv(\"../Data/immo_data_column_description.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d8c8f1-96be-4726-9aba-4a9ca55508f2",
   "metadata": {},
   "source": [
    "## Methods for data preprocessing and model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4851f20-f4c7-4ab3-b727-ce537c55157a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T15:01:00.758077Z",
     "start_time": "2024-11-21T15:01:00.753339Z"
    }
   },
   "outputs": [],
   "source": [
    "def drop_columns(df):\n",
    "    \"\"\" Remove (supposedly) unimportant columns \"\"\"\n",
    "    return df.drop(\n",
    "        [\n",
    "            \"scoutId\",\n",
    "            \"houseNumber\",\n",
    "            \"geo_bln\",\n",
    "            \"geo_krs\",\n",
    "            \"geo_plz\",\n",
    "            \"date\",\n",
    "            \"street\",\n",
    "            \"streetPlain\",\n",
    "            \"description\",\n",
    "            \"facilities\",\n",
    "            \"regio3\",\n",
    "            \"firingTypes\",\n",
    "            \"telekomHybridUploadSpeed\",\n",
    "            \"totalRent\",\n",
    "            \"baseRentRange\",\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "\n",
    "def remove_outliers(df, lower_limit=0.005, upper_limit=0.995):\n",
    "    \"\"\" Removing the (lower and upper) outliers \"\"\"\n",
    "    dfc = df.copy()\n",
    "    columns_with_outliers = [\n",
    "        \"serviceCharge\",\n",
    "        \"yearConstructed\",\n",
    "        \"noParkSpaces\",\n",
    "        \"baseRent\",\n",
    "        \"livingSpace\",\n",
    "        \"noRooms\",\n",
    "        \"floor\",\n",
    "        \"numberOfFloors\",\n",
    "        \"heatingCosts\",\n",
    "        \"lastRefurbish\",\n",
    "    ]\n",
    "\n",
    "    #For each column we keep: Data that are < (99.5% quantile) and > (0.5% quantile) OR that are NaN (we will deal with this later). \n",
    "    upper_limits = df[columns_with_outliers].quantile(upper_limit)\n",
    "    lower_limits = df[columns_with_outliers].quantile(lower_limit)\n",
    "\n",
    "    for colname in columns_with_outliers:\n",
    "        col = dfc[colname]\n",
    "        dfc = dfc[\n",
    "            ((col <= upper_limits[colname]) & (col >= lower_limits[colname]))\n",
    "            | col.isna()\n",
    "            ]\n",
    "    return dfc\n",
    "\n",
    "\n",
    "def remove_rows_with_NaN_target(df):\n",
    "    \"\"\" Removing the records without a label \"\"\"\n",
    "    return df[df[\"baseRent\"].isna() == False]\n",
    "\n",
    "\n",
    "def impute_NaNs(df):\n",
    "    \"\"\" Replacing NaNs with mean or most frequent \"\"\"\n",
    "    dfc = df.copy()\n",
    "    categorical_columns = dfc.select_dtypes(exclude=np.number).columns\n",
    "    imp_freq = SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\")\n",
    "    #dfc.loc[:, categorical_columns] = imp_freq.fit_transform(dfc[categorical_columns])\n",
    "    dfc[categorical_columns] = imp_freq.fit_transform(dfc[categorical_columns])\n",
    "\n",
    "    numeric_columns = dfc.select_dtypes(include=np.number).columns\n",
    "    imp_mean = SimpleImputer(missing_values=np.nan, strategy=\"mean\")\n",
    "    #dfc.loc[:, numeric_columns] = imp_mean.fit_transform(dfc[numeric_columns])\n",
    "    dfc[numeric_columns] = imp_mean.fit_transform(dfc[numeric_columns])\n",
    "    return dfc\n",
    "\n",
    "\n",
    "def print_evaluation(pipeline_or_model, X_train, X_test, y_train, y_test, y_train_pred, y_test_pred, feature_names):\n",
    "    \"\"\" Output of R2 value, MSE and MAE for training and test set \"\"\"\n",
    "    r2_train = r2_score(y_train, y_train_pred)\n",
    "    rmse_train = math.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "\n",
    "    r2_test = r2_score(y_test, y_test_pred)\n",
    "    rmse_test = math.sqrt(mean_squared_error(y_test, y_test_pred));\n",
    "    mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "    print(\n",
    "        f\"{pipeline_or_model} Evaluation:\\n\"\n",
    "        f\"{'':6} {'R²':>10} | {'RMSE':>14} | {'MAE':>10} | {'rows':>8} | {'columns':>8}\\n\"\n",
    "        f\"{'Train':6} {r2_train:10.5f} | {rmse_train:14.2f} | {mae_train:10.2f} | {X_train.shape[0]:8} | {X_train.shape[1]:8}\\n\"\n",
    "        f\"{'Test':6} {r2_test:10.5f} | {rmse_test:14.2f} | {mae_test:10.2f} | {X_test.shape[0]:8} | {X_test.shape[1]:8}\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934c6e2e-617f-4869-830c-555181b8f47c",
   "metadata": {},
   "source": [
    "## 1.Model. The same as in Exercise 06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "016fed6a-366d-41a2-83bc-e012c697c8c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T15:01:09.743502Z",
     "start_time": "2024-11-21T15:01:00.759418Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression() Evaluation:\n",
      "               R² |           RMSE |        MAE |     rows |  columns\n",
      "Train     0.84047 |         163.46 |     106.33 |   207773 |      518\n",
      "Test      0.84079 |         164.57 |     107.07 |    51944 |      518\n"
     ]
    }
   ],
   "source": [
    "# Data pre-processing\n",
    "df_reduced = drop_columns(df)\n",
    "df_reduced = remove_outliers(df_reduced)\n",
    "df_reduced = remove_rows_with_NaN_target(df_reduced)\n",
    "df_reduced = impute_NaNs(df_reduced)\n",
    "df_reduced = pd.get_dummies(df_reduced)\n",
    "y = df_reduced.pop(\"baseRent\")\n",
    "\n",
    "# Training-Test-Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_reduced, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Training\n",
    "model_lr = linear_model.LinearRegression()\n",
    "model_lr.fit(X_train, y_train)\n",
    "y_train_pred = model_lr.predict(X_train)\n",
    "y_test_pred = model_lr.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print_evaluation(model_lr, X_train, X_test, y_train, y_test, y_train_pred, y_test_pred,\n",
    "                 feature_names=df_reduced.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61aa63a5-aa11-48dd-92be-a70d0ddb9b16",
   "metadata": {},
   "source": [
    "## 2. Model: Without outlier distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7297b5c1-0b93-461a-bf90-1671b13b2353",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T15:01:18.046876Z",
     "start_time": "2024-11-21T15:01:09.750602Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression() Evaluation:\n",
      "               R² |           RMSE |        MAE |     rows |  columns\n",
      "Train     0.01653 |       21659.03 |     334.93 |   215080 |      518\n",
      "Test    -47.56742 |        3638.43 |     297.75 |    53770 |      518\n"
     ]
    }
   ],
   "source": [
    "# Data pre-processing\n",
    "df_reduced = drop_columns(df)\n",
    "df_reduced = remove_rows_with_NaN_target(df_reduced)\n",
    "df_reduced = impute_NaNs(df_reduced)\n",
    "df_reduced = pd.get_dummies(df_reduced)\n",
    "y = df_reduced.pop(\"baseRent\")\n",
    "\n",
    "# Training-Test-Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_reduced, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Training\n",
    "model_lr = linear_model.LinearRegression()\n",
    "model_lr.fit(X_train, y_train)\n",
    "y_train_pred = model_lr.predict(X_train)\n",
    "y_test_pred = model_lr.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print_evaluation(model_lr, X_train, X_test, y_train, y_test, y_train_pred, y_test_pred,\n",
    "                 feature_names=df_reduced.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663a69b7-9844-4374-90e1-da2a01b2d8be",
   "metadata": {},
   "source": [
    "If the outliers are not removed, the model is extremely poor even on the training data. Since some outliers here are several orders of magnitude above the \"normal\" range (15 million EUR rent!), a few points are enough to make the model completely unsuitable for forecasting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befff765-c43f-40a8-bbc6-6aac6195ba04",
   "metadata": {},
   "source": [
    "## 3. Model: Regionally restricted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ac899c2-6629-4902-b483-eee8659773f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T15:01:18.429975Z",
     "start_time": "2024-11-21T15:01:18.064649Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression() Evaluation:\n",
      "               R² |           RMSE |        MAE |     rows |  columns\n",
      "Train     0.76241 |         214.60 |     154.29 |    12447 |      127\n",
      "Test      0.74666 |         222.64 |     158.49 |     3112 |      127\n"
     ]
    }
   ],
   "source": [
    "# Data pre-processing\n",
    "df_reduced = drop_columns(df[df[\"regio1\"] == \"Baden_Württemberg\"])\n",
    "#df_reduced = drop_columns(df[df[\"regio2\"]==\"Karlsruhe\"])\n",
    "df_reduced = remove_outliers(df_reduced)\n",
    "df_reduced = remove_rows_with_NaN_target(df_reduced)\n",
    "df_reduced = impute_NaNs(df_reduced)\n",
    "df_reduced = pd.get_dummies(df_reduced)\n",
    "y = df_reduced.pop(\"baseRent\")\n",
    "\n",
    "# Training-Test-Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_reduced, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Training\n",
    "model_lr = linear_model.LinearRegression()\n",
    "model_lr.fit(X_train, y_train)\n",
    "y_train_pred = model_lr.predict(X_train)\n",
    "y_test_pred = model_lr.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print_evaluation(model_lr, X_train, X_test, y_train, y_test, y_train_pred, y_test_pred,\n",
    "                 feature_names=df_reduced.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9262d2-826f-40d8-86e7-5aaa520fe578",
   "metadata": {},
   "source": [
    "The overall model quality decreases somewhat. If the region is chosen too small, there is too little data and we observe overfitting (result on test data significantly worse than on training data). Since there are fewer data points, the training is significantly faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a83420e-ec69-484a-86f9-5a3fd5d7e71f",
   "metadata": {},
   "source": [
    "## 4. Model: Restriction to $K$ features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8ef4fd1-afc3-459d-81c5-9fbdefb15e2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T15:01:21.277979Z",
     "start_time": "2024-11-21T15:01:18.433857Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression() Evaluation:\n",
      "               R² |           RMSE |        MAE |     rows |  columns\n",
      "Train     0.79481 |         185.31 |     123.01 |   207773 |       50\n",
      "Test      0.79633 |         186.40 |     123.68 |    51944 |       50\n"
     ]
    }
   ],
   "source": [
    "# Data pre-processing\n",
    "df_reduced = drop_columns(df)\n",
    "df_reduced = remove_outliers(df_reduced)\n",
    "df_reduced = remove_rows_with_NaN_target(df_reduced)\n",
    "df_reduced = impute_NaNs(df_reduced)\n",
    "df_reduced = pd.get_dummies(df_reduced)\n",
    "y = df_reduced.pop(\"baseRent\")\n",
    "\n",
    "# Training-Test-Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_reduced, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# By SelectKBest we choose the 50 most promising features\n",
    "feature_selection = SelectKBest(f_regression, k=50)\n",
    "X_train = feature_selection.fit_transform(X_train, y_train)\n",
    "X_test = feature_selection.transform(X_test)\n",
    "\n",
    "# Training\n",
    "model_lr = linear_model.LinearRegression()\n",
    "model_lr.fit(X_train, y_train)\n",
    "y_train_pred = model_lr.predict(X_train)\n",
    "y_test_pred = model_lr.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print_evaluation(model_lr, X_train, X_test, y_train, y_test, y_train_pred, y_test_pred,\n",
    "                 feature_names=df_reduced.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1a7087-f9af-44bf-9d02-76df90bd857f",
   "metadata": {},
   "source": [
    "The R² value drops from 0.84 to 0.79, but we only need 10% of the features. The smaller you choose the $K$, the worse the model becomes overall. Note: `SelectKBest` is only a *heuristic*, it is not guaranteed that the resulting model with 50 features is the best of all models with 50 features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee148997-94c4-48e5-98fd-408ec56b6195",
   "metadata": {},
   "source": [
    "## 5. Model: Additional features through combination of existing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d4bab02-d9d6-465b-9821-6a8676e2e555",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T15:01:39.408200Z",
     "start_time": "2024-11-21T15:01:21.281602Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression() Evaluation:\n",
      "               R² |           RMSE |        MAE |     rows |  columns\n",
      "Train     0.86526 |         150.16 |      95.47 |   207773 |      937\n",
      "Test      0.86528 |         151.60 |      96.48 |    51944 |      937\n"
     ]
    }
   ],
   "source": [
    "# Data pre-processing\n",
    "df_reduced = drop_columns(df)\n",
    "df_reduced = remove_outliers(df_reduced)\n",
    "df_reduced = remove_rows_with_NaN_target(df_reduced)\n",
    "df_reduced = impute_NaNs(df_reduced)\n",
    "df_reduced = pd.get_dummies(df_reduced)\n",
    "y = df_reduced.pop(\"baseRent\")\n",
    "\n",
    "# A transformer that generates interaction features of (up to) degree 2\n",
    "pf = PolynomialFeatures(degree=2, interaction_only=True)\n",
    "# Interaction features: kreis x living Space\n",
    "kreis_columns = \"regio2_\" + df[\"regio2\"].unique()\n",
    "for col in kreis_columns:\n",
    "    features = pf.fit_transform(df_reduced[[col, \"livingSpace\"]])\n",
    "    df_reduced[col + \"_livingSpace\"] = features[:, -1]\n",
    "\n",
    "# Training-Test-Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_reduced, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Training\n",
    "model_lr = linear_model.LinearRegression()\n",
    "model_lr.fit(X_train, y_train)\n",
    "y_train_pred = model_lr.predict(X_train)\n",
    "y_test_pred = model_lr.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print_evaluation(model_lr, X_train, X_test, y_train, y_test, y_train_pred, y_test_pred,\n",
    "                 feature_names=df_reduced.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cdb333-3e38-40b7-8501-c2005edfe784",
   "metadata": {},
   "source": [
    "Further useful features (here: Interaction feature between district and living space, i.e. additional rent per additional square metre of living space, whereby this coefficient can vary depending on the district), the model quality can be increased somewhat. We now have 931 features and the computing time for training is now significantly longer than with 512 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "477740e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T15:01:39.413170Z",
     "start_time": "2024-11-21T15:01:39.410599Z"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a246d744-7ba9-49f8-9aff-f2019d712010",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T15:01:39.417094Z",
     "start_time": "2024-11-21T15:01:39.414418Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5c5548e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T15:01:39.429804Z",
     "start_time": "2024-11-21T15:01:39.426487Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccf7bd1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T15:01:39.434577Z",
     "start_time": "2024-11-21T15:01:39.431617Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
